[2023-04-04T19:21:57.027-0400] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: air_quality_pipeline.run_consumer manual__2023-04-04T23:16:03.639029+00:00 [queued]>
[2023-04-04T19:21:57.035-0400] {taskinstance.py:1084} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: air_quality_pipeline.run_consumer manual__2023-04-04T23:16:03.639029+00:00 [queued]>
[2023-04-04T19:21:57.035-0400] {taskinstance.py:1282} INFO - 
--------------------------------------------------------------------------------
[2023-04-04T19:21:57.035-0400] {taskinstance.py:1283} INFO - Starting attempt 2 of 2
[2023-04-04T19:21:57.035-0400] {taskinstance.py:1284} INFO - 
--------------------------------------------------------------------------------
[2023-04-04T19:21:57.042-0400] {taskinstance.py:1303} INFO - Executing <Task(BashOperator): run_consumer> on 2023-04-04 23:16:03.639029+00:00
[2023-04-04T19:21:57.048-0400] {standard_task_runner.py:55} INFO - Started process 71861 to run task
[2023-04-04T19:21:57.053-0400] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'air_quality_pipeline', 'run_consumer', 'manual__2023-04-04T23:16:03.639029+00:00', '--job-id', '93', '--raw', '--subdir', 'DAGS_FOLDER/__pycache__/pipeline.py', '--cfg-path', '/var/folders/8z/pwbx6b3n3mnd1g1f397fv1100000gn/T/tmpu26yytuz']
[2023-04-04T19:21:57.054-0400] {standard_task_runner.py:83} INFO - Job 93: Subtask run_consumer
[2023-04-04T19:21:57.097-0400] {task_command.py:388} INFO - Running <TaskInstance: air_quality_pipeline.run_consumer manual__2023-04-04T23:16:03.639029+00:00 [running]> on host srivardhans-air.starry-inc.net
[2023-04-04T19:21:57.136-0400] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=yourname
AIRFLOW_CTX_DAG_ID=air_quality_pipeline
AIRFLOW_CTX_TASK_ID=run_consumer
AIRFLOW_CTX_EXECUTION_DATE=2023-04-04T23:16:03.639029+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-04-04T23:16:03.639029+00:00
[2023-04-04T19:21:57.137-0400] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/8z/pwbx6b3n3mnd1g1f397fv1100000gn/T
[2023-04-04T19:21:57.138-0400] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python3 "/Users/srivardhan/Desktop/Masters/Masters Project/Kafka Implementation/kafka-pipeline-examples/producer-consumer/consumer.py"']
[2023-04-04T19:21:57.147-0400] {subprocess.py:86} INFO - Output:
[2023-04-04T19:22:09.450-0400] {subprocess.py:93} INFO - Setting default log level to "WARN".
[2023-04-04T19:22:09.452-0400] {subprocess.py:93} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2023-04-04T19:22:09.646-0400] {subprocess.py:93} INFO - 23/04/04 19:22:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-04-04T19:22:13.547-0400] {subprocess.py:93} INFO - [Stage 0:>                                                          (0 + 8) / 8]                                                                                Connecting to consumer ...
[2023-04-04T19:22:13.548-0400] {subprocess.py:93} INFO - Start of kafka session
[2023-04-04T19:22:13.549-0400] {subprocess.py:93} INFO - No messages received, stopping consumer
[2023-04-04T19:22:13.549-0400] {subprocess.py:93} INFO - Start of spark session
[2023-04-04T19:22:13.549-0400] {subprocess.py:93} INFO - root
[2023-04-04T19:22:13.549-0400] {subprocess.py:93} INFO - 
[2023-04-04T19:22:13.549-0400] {subprocess.py:93} INFO - ++
[2023-04-04T19:22:13.550-0400] {subprocess.py:93} INFO - ||
[2023-04-04T19:22:13.550-0400] {subprocess.py:93} INFO - ++
[2023-04-04T19:22:13.550-0400] {subprocess.py:93} INFO - ++
[2023-04-04T19:22:13.550-0400] {subprocess.py:93} INFO - 
[2023-04-04T19:22:13.550-0400] {subprocess.py:93} INFO - Error : Column 'sensor.sensor_index' does not exist. Did you mean one of the following? [];
[2023-04-04T19:22:13.551-0400] {subprocess.py:93} INFO - 'Project ['sensor.sensor_index, 'sensor.latitude, 'sensor.longitude, 'sensor.pressure, 'sensor.`pm2.5_atm_a`, 'sensor.`pm2.5_atm_b`, 'sensor.humidity_a, 'sensor.temperature_a, 'sensor.location_type, 'sensor.`pm1.0_atm_a`, 'sensor.`pm1.0_atm_b`, 'sensor.scattering_coefficient_a, 'sensor.scattering_coefficient_b]
[2023-04-04T19:22:13.551-0400] {subprocess.py:93} INFO - +- LogicalRDD false
[2023-04-04T19:22:13.551-0400] {subprocess.py:93} INFO - 
[2023-04-04T19:22:13.551-0400] {subprocess.py:93} INFO - Traceback (most recent call last):
[2023-04-04T19:22:13.551-0400] {subprocess.py:93} INFO -   File "/Users/srivardhan/Desktop/Masters/Masters Project/Kafka Implementation/kafka-pipeline-examples/producer-consumer/consumer.py", line 68, in <module>
[2023-04-04T19:22:13.551-0400] {subprocess.py:93} INFO -     col('sensor.`scattering_coefficient_b`')
[2023-04-04T19:22:13.552-0400] {subprocess.py:93} INFO -   File "/Users/srivardhan/opt/anaconda3/envs/kafka-pipeline/lib/python3.7/site-packages/pyspark/sql/dataframe.py", line 2023, in select
[2023-04-04T19:22:13.552-0400] {subprocess.py:93} INFO -     jdf = self._jdf.select(self._jcols(*cols))
[2023-04-04T19:22:13.552-0400] {subprocess.py:93} INFO -   File "/Users/srivardhan/opt/anaconda3/envs/kafka-pipeline/lib/python3.7/site-packages/py4j/java_gateway.py", line 1322, in __call__
[2023-04-04T19:22:13.552-0400] {subprocess.py:93} INFO -     answer, self.gateway_client, self.target_id, self.name)
[2023-04-04T19:22:13.552-0400] {subprocess.py:93} INFO -   File "/Users/srivardhan/opt/anaconda3/envs/kafka-pipeline/lib/python3.7/site-packages/pyspark/sql/utils.py", line 196, in deco
[2023-04-04T19:22:13.553-0400] {subprocess.py:93} INFO -     raise converted from None
[2023-04-04T19:22:13.553-0400] {subprocess.py:93} INFO - pyspark.sql.utils.AnalysisException: Column 'sensor.sensor_index' does not exist. Did you mean one of the following? [];
[2023-04-04T19:22:13.553-0400] {subprocess.py:93} INFO - 'Project ['sensor.sensor_index, 'sensor.latitude, 'sensor.longitude, 'sensor.pressure, 'sensor.`pm2.5_atm_a`, 'sensor.`pm2.5_atm_b`, 'sensor.humidity_a, 'sensor.temperature_a, 'sensor.location_type, 'sensor.`pm1.0_atm_a`, 'sensor.`pm1.0_atm_b`, 'sensor.scattering_coefficient_a, 'sensor.scattering_coefficient_b]
[2023-04-04T19:22:13.553-0400] {subprocess.py:93} INFO - +- LogicalRDD false
[2023-04-04T19:22:13.553-0400] {subprocess.py:93} INFO - 
[2023-04-04T19:22:14.145-0400] {subprocess.py:97} INFO - Command exited with return code 1
[2023-04-04T19:22:14.161-0400] {taskinstance.py:1775} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/srivardhan/opt/anaconda3/envs/kafka-pipeline/lib/python3.7/site-packages/airflow/operators/bash.py", line 197, in execute
    f"Bash command failed. The command returned a non-zero exit code {result.exit_code}."
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2023-04-04T19:22:14.164-0400] {taskinstance.py:1326} INFO - Marking task as FAILED. dag_id=air_quality_pipeline, task_id=run_consumer, execution_date=20230404T231603, start_date=20230404T232157, end_date=20230404T232214
[2023-04-04T19:22:14.173-0400] {standard_task_runner.py:105} ERROR - Failed to execute job 93 for task run_consumer (Bash command failed. The command returned a non-zero exit code 1.; 71861)
[2023-04-04T19:22:14.208-0400] {local_task_job.py:212} INFO - Task exited with return code 1
[2023-04-04T19:22:14.231-0400] {taskinstance.py:2585} INFO - 0 downstream tasks scheduled from follow-on schedule check
